{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量\n",
    "Tensor(张量)类似于Numpy的ndarray,但是还可以在GPU上使用加速计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.1704e-41],\n",
      "        [0.0000e+00, 2.2369e+08, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [       nan,        nan, 8.7449e-37]])\n"
     ]
    }
   ],
   "source": [
    "#床架一个没有初始化的5*3的矩阵\n",
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0147, 0.7039, 0.0326],\n",
      "        [0.9560, 0.6162, 0.3584],\n",
      "        [0.2005, 0.3889, 0.7750],\n",
      "        [0.7529, 0.2878, 0.0333],\n",
      "        [0.4974, 0.6801, 0.8139]])\n"
     ]
    }
   ],
   "source": [
    "#创建一个随机初始化矩阵\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "#创建一个填满0且类型是long的矩阵\n",
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "#直接从数据构建张量\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[0.1492, 0.4448, 0.2332],\n",
      "        [0.5647, 0.3428, 0.3211],\n",
      "        [0.2747, 0.3288, 0.3825],\n",
      "        [0.1067, 0.4612, 0.6010],\n",
      "        [0.3897, 0.2342, 0.0221]])\n"
     ]
    }
   ],
   "source": [
    "#根据已有的tensor建立新的tensor,除非提供新的值,斗则这些方法将重用输入张量的属性\n",
    "x = x.new_ones(5, 3, dtype = torch.double)\n",
    "print(x)\n",
    "\n",
    "x = torch.rand_like(x, dtype = torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "#获取张量的形状\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> torch.Size本质上还是tuple,支持tuple的一起操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1492, 0.4448, 0.2332],\n",
      "        [0.5647, 0.3428, 0.3211],\n",
      "        [0.2747, 0.3288, 0.3825],\n",
      "        [0.1067, 0.4612, 0.6010],\n",
      "        [0.3897, 0.2342, 0.0221]]) tensor([[0.6788, 0.9013, 0.9111],\n",
      "        [0.0757, 0.6049, 0.2100],\n",
      "        [0.6781, 0.2057, 0.5511],\n",
      "        [0.4034, 0.8253, 0.5214],\n",
      "        [0.9979, 0.8502, 0.5180]])\n",
      "tensor([[0.6788, 0.9013, 0.9111],\n",
      "        [0.0757, 0.6049, 0.2100],\n",
      "        [0.6781, 0.2057, 0.5511],\n",
      "        [0.4034, 0.8253, 0.5214],\n",
      "        [0.9979, 0.8502, 0.5180]])\n",
      "第一种形式:x+y\n",
      "tensor([[0.8279, 1.3462, 1.1444],\n",
      "        [0.6404, 0.9477, 0.5311],\n",
      "        [0.9528, 0.5345, 0.9336],\n",
      "        [0.5101, 1.2865, 1.1223],\n",
      "        [1.3876, 1.0844, 0.5402]])\n",
      "第二种形式:torch.add(x, y)\n",
      "tensor([[0.8279, 1.3462, 1.1444],\n",
      "        [0.6404, 0.9477, 0.5311],\n",
      "        [0.9528, 0.5345, 0.9336],\n",
      "        [0.5101, 1.2865, 1.1223],\n",
      "        [1.3876, 1.0844, 0.5402]])\n",
      "第三种形式:给定一个输出张量作为参数 out=result\n",
      "tensor([[0.8279, 1.3462, 1.1444],\n",
      "        [0.6404, 0.9477, 0.5311],\n",
      "        [0.9528, 0.5345, 0.9336],\n",
      "        [0.5101, 1.2865, 1.1223],\n",
      "        [1.3876, 1.0844, 0.5402]])\n",
      "第四种形式:原位/原地操作(in-place)\n",
      "tensor([[0.8279, 1.3462, 1.1444],\n",
      "        [0.6404, 0.9477, 0.5311],\n",
      "        [0.9528, 0.5345, 0.9336],\n",
      "        [0.5101, 1.2865, 1.1223],\n",
      "        [1.3876, 1.0844, 0.5402]])\n"
     ]
    }
   ],
   "source": [
    "# 加法\n",
    "y = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(y)\n",
    "print(\"第一种形式:x+y\")\n",
    "print(x + y)\n",
    "print(\"第二种形式:torch.add(x, y)\")\n",
    "print(torch.add(x, y))\n",
    "print(\"第三种形式:给定一个输出张量作为参数 out=result\")\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "print(\"第四种形式:原位/原地操作(in-place)\")\n",
    "print(y.add_(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注意:\n",
    "任何一个in-place改变张量的操作后面都固定一个_。例如x.copy_(y)、x.t_()将更改x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1492, 0.4448, 0.2332],\n",
      "        [0.5647, 0.3428, 0.3211],\n",
      "        [0.2747, 0.3288, 0.3825],\n",
      "        [0.1067, 0.4612, 0.6010],\n",
      "        [0.3897, 0.2342, 0.0221]])\n",
      "tensor([0.4448, 0.3428, 0.3288, 0.4612, 0.2342])\n"
     ]
    }
   ],
   "source": [
    "# 可以使用像Numpy使用索引进行操作\n",
    "print(x)\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8648, -0.3935, -0.1712, -0.5305],\n",
      "        [-1.5016,  0.5812,  0.5898, -1.1872],\n",
      "        [-0.4456, -1.2171, -0.5073,  0.1688],\n",
      "        [ 1.7078, -0.8416, -0.7522,  0.9512]])\n",
      "tensor([-0.8648, -0.3935, -0.1712, -0.5305, -1.5016,  0.5812,  0.5898, -1.1872,\n",
      "        -0.4456, -1.2171, -0.5073,  0.1688,  1.7078, -0.8416, -0.7522,  0.9512])\n",
      "tensor([[-0.8648, -0.3935, -0.1712, -0.5305, -1.5016,  0.5812,  0.5898, -1.1872],\n",
      "        [-0.4456, -1.2171, -0.5073,  0.1688,  1.7078, -0.8416, -0.7522,  0.9512]])\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# 改变形状:使用torch.view\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # -1是从其他尺寸推断的\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6300])\n",
      "-0.6300126314163208\n"
     ]
    }
   ],
   "source": [
    "# 如果是仅包含一个元素的tensor，可以使用.item()来得到对应的python数值\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 桥接NumPy\n",
    "Torch张量和NumPy数组将共享它们底层内存位置,因此一个改变时另一个也同时改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 将torch的Tensor转为NumPy的数组\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# 测试NumPy数组改变数值\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 将NumPy数组装维tensor张量\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CPU上的所有张量(CharTensor除外)都支持与Numpy的相互转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA上的张量\n",
    "张量可以使用.to方法移动到任何设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当GPU可用时\n",
    "# 将使用torch.device来将tensor移入和移出GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # 直接在GPU上创建tensor\n",
    "    x = x.to(device)                       # 或者使用`.to(\"cuda\")`方法\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # `.to`也能在移动时改变dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd:自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中，所有神经网络的核心是 `autograd` 包\n",
    "`autograd` 包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义(define-by-run）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的.\n",
    "## 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor` 是这个包的核心类。如果设置它的属性 `.requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 `.backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性.\n",
    "\n",
    "要阻止一个张量被跟踪历史，可以调用 `.detach()` 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。\n",
    "\n",
    "为了防止跟踪历史记录(和使用内存），可以将代码块包装在 `with torch.no_grad()`: 中。在评估模型时特别有用，因为模型可能具有 `requires_grad = True` 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。\n",
    "\n",
    "还有一个类对于autograd的实现非常重要：`Function`。\n",
    "\n",
    "`Tensor` 和 `Function` 互相连接生成了一个无圈图(acyclic graph)，它编码了完整的计算历史。每个张量都有一个 `.grad_fn` 属性，该属性引用了创建 `Tensor` 自身的`Function`(除非这个张量是用户手动创建的，即这个张量的 `grad_fn` 是 `None` )。\n",
    "\n",
    "如果需要计算导数，可以在 `Tensor` 上调用 `.backward()`。如果 `Tensor` 是一个标量(即它包含一个元素的数据），则不需要为 `backward()` 指定任何参数，但是如果它有更多的元素，则需要指定一个 `gradient` 参数，该参数是形状匹配的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#创建一个张量并设置requires_grad=True用来追踪其计算历史\n",
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#对这个张量做一次运算\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x12126bd30>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#对y进行其他操作\n",
    "z = y * y* 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.requires_grad_(...)` 原地改变了现有张量的 `requires_grad` 标志。如果没有指定的话，默认输入的这个标志是 `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x12113c7c0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度\n",
    "现在开始进行反向传播，因为 `out` 是一个标量，因此 `out.backward()` 和 `out.backward(torch.tensor(1.))` 等价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 输出导数 d(out)/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
